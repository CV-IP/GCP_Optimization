# Deep Global Generalized Gaussian Networks
This is an code implementation of CVPR20 paper ([What Deep CNNs Benefit from Global Covariance Pooling: An Optimization Perspective]()), created by [Qilong Wang](https://csqlwang.github.io/homepage/) and Li Zhang.

## Introduction
Recent works have demonstrated that global covariance pooling (GCP) has the ability to improve performance of deep convolutional neural networks (CNNs) on visual classification task. Despite considerable advance, the reasons on effectiveness of GCP on deep CNNs have not been well studied. In this paper, we make an attempt to understand
what deep CNNs benefit from GCP in a viewpoint of optimization. Specifically, we explore the effect of GCP on deep CNNs in terms of the Lipschitzness of optimization loss and the predictiveness of gradients, and show that GCP can make the optimization landscape more smooth and the gradients more predictive. Furthermore, we discuss the connection between GCP and second-order optimization for deep CNNs. More importantly, above findings can account for several merits of covariance pooling for training deep CNNs that have not been recognized previously or fully explored, including significant acceleration of network convergence (i.e., the networks trained with GCP can support rapid decay of learning rates, achieving favorable
performance while significantly reducing number of training epochs), stronger robustness to distorted examples generated by image corruptions and perturbations, and good generalization ability to different vision tasks, e.g., object detection and instance segmentation. We conduct extensive experiments using various deep CNN architectures on diversified tasks, and the results provide strong support to our findings.

## Overview
![Net]()

## Citation


## Our environments

- OS: Ubuntu 16.04
- CUDA: 9.0/10.0
- Toolkit: PyTorch 1.3/1.4
- GPU: GTX 2080Ti/TiTan XP

## Installation

1.pytorch installation following [pytorch.org](https://pytorch.org/)

2.`conda install numpy`

3.`conda install torchvision`

3.`conda install pillow==6.1`

## Usage

1. Training models (except ShuffleNetV2) on ImageNet: In floder `src` , run ` sh ./scripts/train/train.sh `

2. Training ShuffleNetV2 models on ImageNet: In floder `src` , run ` sh ./scripts/train/train_shufflenet.sh `

3. Testing models on ImageNet:  In floder `src` , run ` sh ./scripts/val/val.sh `

*Note that you need to modify  the `dataset path` or `model name` in `train.sh` or `val.sh` for fitting your configurations, and descriptions on all parameters can be found in file `./scripts/readme.txt`.

## Main Results and Models 

### MobileNetV2
|Models|Top-1 acc.(%)|Top-5 acc.(%)|BaiduDrive(models)|Extract code|GoogleDrive|
|:----:|:-----------:|:-----------:|:----------------:|:----------:|:---------:|
|MobileNetV2_GAP_LRnorm|71.62|90.18|[MobileNetV2_GAP_LRnorm]()||[MobileNetV2_GAP_LRnorm]()|
|MobileNetV2_GAP_LRfast|69.29|89.01|[MobileNetV2_GAP_LRfast]()||[MobileNetV2_GAP_LRfast]()|
|MobileNetV2_GAP_LRadju|71.27|90.08|[MobileNetV2_GAP_LRadju]()||[MobileNetV2_GAP_LRadju]()|
|MobileNetV2_GCP_LRnorm|74.39|91.86|[MobileNetV2_GCP_LRnorm]()||[MobileNetV2_GCP_LRnorm]()|
|MobileNetV2_GCP_LRfast|72.45|90.51|[MobileNetV2_GCP_LRfast]()||[MobileNetV2_GCP_LRfast]()|
|MobileNetV2_GCP_LRadju|73.97|91.53|[MobileNetV2_GCP_LRadju]()||[MobileNetV2_GCP_LRadju]()|
|MobileNetV2_GCP_LRnorm_128|73.28|91.26|[MobileNetV2_GCP_LRnorm_128]()||[MobileNetV2_GCP_LRnorm_128]()|
|MobileNetV2_GCP_LRadju_128|72.58|90.87|[MobileNetV2_GCP_LRadju_128]()||[MobileNetV2_GCP_LRadju_128]()|

### ShuffleNetV2
|Models|Top-1 acc.(%)|Top-5 acc.(%)|BaiduDrive(models)|Extract code|GoogleDrive|
|:----:|:-----------:|:-----------:|:----------------:|:----------:|:---------:|
|ShuffleNetV2_GAP_LRnorm|67.96|87.84|[ShuffleNetV2_GAP_LRnorm]()||[ShuffleNetV2_GAP_LRnorm]()|
|ShuffleNetV2_GAP_LRfast|66.13|86.54|[ShuffleNetV2_GAP_LRfast]()||[ShuffleNetV2_GAP_LRfast]()|
|ShuffleNetV2_GAP_LRadju|67.15|87.35|[ShuffleNetV2_GAP_LRadju]()||[ShuffleNetV2_GAP_LRadju]()|
|ShuffleNetV2_GCP_LRnorm|71.83|90.04|[ShuffleNetV2_GCP_LRnorm]()||[ShuffleNetV2_GCP_LRnorm]()|
|ShuffleNetV2_GCP_LRfast|70.29|89.00|[ShuffleNetV2_GCP_LRfast]()||[ShuffleNetV2_GCP_LRfast]()|
|ShuffleNetV2_GCP_LRadju|71.17|89.74|[ShuffleNetV2_GCP_LRadju]()||[ShuffleNetV2_GCP_LRadju]()|

### ResNet18
|Models|Top-1 acc.(%)|Top-5 acc.(%)|BaiduDrive(models)|Extract code|GoogleDrive|
|:----:|:-----------:|:-----------:|:----------------:|:----------:|:---------:|
|ResNet18_GAP_LRnorm|70.47|89.59|[ResNet18_GAP_LRnorm]()||[ResNet18_GAP_LRnorm]()|
|ResNet18_GAP_LRfast|66.02|86.69|[ResNet18_GAP_LRfast]()||[ResNet18_GAP_LRfast]()|
|ResNet18_GAP_LRadju|69.62|89.00|[ResNet18_GAP_LRadju]()||[ResNet18_GAP_LRadju]()|
|ResNet18_GCP_LRnorm|75.48|92.23|[ResNet18_GCP_LRnorm]()||[ResNet18_GCP_LRnorm]()|
|ResNet18_GCP_LRfast|72.02|89.97|[ResNet18_GCP_LRfast]()||[ResNet18_GCP_LRfast]()|
|ResNet18_GCP_LRadju|74.86|91.81|[ResNet18_GCP_LRadju]()||[ResNet18_GCP_LRadju]()|

### ResNet34
|Models|Top-1 acc.(%)|Top-5 acc.(%)|BaiduDrive(models)|Extract code|GoogleDrive|
|:----:|:-----------:|:-----------:|:----------------:|:----------:|:---------:|
|ResNet34_GAP_LRnorm|74.19|91.60|[ResNet34_GAP_LRnorm]()||[ResNet34_GAP_LRnorm]()|
|ResNet34_GAP_LRfast|69.88|89.25|[ResNet34_GAP_LRfast]()||[ResNet34_GAP_LRfast]()|
|ResNet34_GAP_LRadju|73.13|91.14|[ResNet34_GAP_LRadju]()||[ResNet34_GAP_LRadju]()|
|ResNet34_GCP_LRnorm|77.11|93.33|[ResNet34_GCP_LRnorm]()||[ResNet34_GCP_LRnorm]()|
|ResNet34_GCP_LRfast|73.88|91.42|[ResNet34_GCP_LRfast]()||[ResNet34_GCP_LRfast]()|
|ResNet34_GCP_LRadju|76.81|93.04|[ResNet34_GCP_LRadju]()||[ResNet34_GCP_LRadju]()|

### ResNet50
|Models|Top-1 acc.(%)|Top-5 acc.(%)|BaiduDrive(models)|Extract code|GoogleDrive|
|:----:|:-----------:|:-----------:|:----------------:|:----------:|:---------:|
|ResNet50_GAP_LRnorm|76.02|92.97|[ResNet50_GAP_LRnorm]()||[ResNet50_GAP_LRnorm]()|
|ResNet50_GAP_LRfast|71.08|90.04|[ResNet50_GAP_LRfast]()||[ResNet50_GAP_LRfast]()|
|ResNet50_GAP_LRadju|75.32|92.47|[ResNet50_GAP_LRadju]()||[ResNet50_GAP_LRadju]()|
|ResNet50_GCP_LRnorm|78.56|94.09|[ResNet50_GCP_LRnorm]()||[ResNet50_GCP_LRnorm]()|
|ResNet50_GCP_LRfast|75.31|92.11|[ResNet50_GCP_LRfast]()||[ResNet50_GCP_LRfast]()|
|ResNet50_GCP_LRadju|78.03|93.95|[ResNet50_GCP_LRadju]()||[ResNet50_GCP_LRadju]()|
|ResNet50_GCP_LRnorm_128|78.02|94.02|[ResNet50_GCP_LRnorm_128]()||[ResNet50_GCP_LRnorm_128]()|
|ResNet50_GCP_LRadju_128|77.72|93.73|[ResNet50_GCP_LRadju_128]()||[ResNet50_GCP_LRadju_128]()|

### ResNet101
|Models|Top-1 acc.(%)|Top-5 acc.(%)|BaiduDrive(models)|Extract code|GoogleDrive|
|:----:|:-----------:|:-----------:|:----------------:|:----------:|:---------:|
|ResNet101_GAP_LRnorm|77.67|93.83|[ResNet101_GAP_LRnorm]()||[ResNet101_GAP_LRnorm]()|
|ResNet101_GAP_LRfast|73.13|91.06|[ResNet101_GAP_LRfast]()||[ResNet101_GAP_LRfast]()|
|ResNet101_GAP_LRadju|77.53|93.53|[ResNet101_GAP_LRadju]()||[ResNet101_GAP_LRadju]()|
|ResNet101_GCP_LRnorm|79.47|94.71|[ResNet101_GCP_LRnorm]()||[ResNet101_GCP_LRnorm]()|
|ResNet101_GCP_LRfast|76.38|92.82|[ResNet101_GCP_LRfast]()||[ResNet101_GCP_LRfast]()|
|ResNet101_GCP_LRadju|79.18|94.47|[ResNet101_GCP_LRadju]()||[ResNet101_GCP_LRadju]()|

*If you would like to evaluate above pre-trained models, please do the following:

1. Download the pre-trained models.

2. Testing on ImageNet: In floder `src` , run ` sh ./scripts/val/val_download.sh` 

## Acknowledgments
We would like to thank the team behind the [iSQRT-COV](https://github.com/jiangtaoxie/fast-MPN-COV) for providing a nice code, and our code is based on it.

## Contact
If you have any questions or suggestions, please feel free to contact us: qlwang@tju.edu.cn; li_zhang@tju.edu.cn.
